\chapter{Discussion}
\label{chap:discussion}

\section{Overall reflection and experience}

Initially the group wanted to keep very high standards for code quality and professionalism. Although all still strove for this throughout the project the team quickly fell behind schedule, therefore sacrificed on professionalism in order to get working features and ensure scalability. The parts of professionalism that suffered was mostly documentation of the process and unit tests. Part of why the documentation did not stand up to what was suggested in the project plan as shown in the appendix \ref{app:projectPlan}, was that the project plan template did not fit highly \gls{agile} workflows like the group followed. The group followed \gls{scrum} and had documented sprint retrospective and sprint planning meetings, but the low communication threshold meant that most planning was done verbally and not through writing. It allowed for rapidly changing ideas and refactoring, but very little code that could be considered final and therefore not considered worth writing tests for. This will make it more difficult for potential \gls{opensource} developers to take over afterwards. Although it is unlikely that others will continue the project it still seemed like a valid compromise.

The low communication threshold meant that those working on relatively complex tasks would ask for a second opinion or ideas for potential solutions. This in turn meant that each group member felt committed and prioritized the more advanced tasks. This was beneficial and helped to solve some of the tasks that could not have been solved by an individual. This also had the negative effects of down prioritizing less complex tasks that could easily give a slight enhancement to the project and that the groups focus could slip. The groups would talk about project unrelated topics more than what was considered optimal and therefore had to spend more time than what was expected. Time log can be found in appendix \ref{app:timelog}.

It was often mentioned that progress was behind schedule set in the project plan, although the schedule comment specifically mentioned that changes were expected and should be embraced. It is good that this pushed for quicker development, but it might also have harmed the scalability of the project and thereby also slowed it down by pushing for features when the plan was not fully formed. 

The group was always aware that the project was complicated and required a lot of research and experimentation, but throughout the development it seemed that group members did not expect the same level of completeness. The product owner had at some occasions described the project as a research project, yet the supervisor mentioned that the project requirements should be written as hard requirements, something that conflicted with both the research based and agile nature of the project.

Overall impressions is as expected, the group knew the project was very ambitious and aimed more for an idea than a realistic goal. This meant the learning outcome would be significant and pushed the team to do better than what was realistic. It was hard work, but still very interesting and as a group there was a genuine belief that all members would want to continue developing the project after the thesis was finished.

\section{Use of Jira}
\newglossaryentry{toggl}{name={Toggl}, description={A time tracking app \cite{wiki:toggl}}}
Throughout the project the team used \gls{jira} for managing the backlog, sprints and the planning of tasks within these sprints. Some tasks at the start were quite large and required Full-Stack development, e.g. "As a user I would like to see classes parsed and visualized in 3D". These tasks are far too big, especially for the group who has very limited experience with technologies used like \gls{antlr} and \gls{js}. The individual group members would also spend a whole week on the same task and this felt quite daunting after a couple of weeks. The group then decided to make these tasks into epics and segment them down into user-stories and sub-tasks. This made the tasks more finely defined and easier to tackle. This made the group members individually more motivated and made the development process feel less daunting, because a member could complete more tasks in a shorter period of time, therefore gain more satisfaction in seeing multiple tasks getting completed. The break-down of the tasks also had the added benefits in making the burn-down charts look more professional and would more accurately represent the work being done. Despite this, underestimation was still a severe problem and some stories were not estimated due to \gls{jira} treating sub-tasks, stories and epics differently. The \gls{planningPoker} tool in \gls{jira} worked well for estimating stories, but had problems with subtask. Subtasks did not have an estimation field by default. This might be because \gls{jira} meant for them to be of trivial size. The team did not use them like that, but added an estimation field, however the \gls{planningPoker} tool did not pick this automatically. In general task estimations were greatly underestimated since it was difficult to see the full length of the task when the task was described orally.

To keep track of the time spent on each task, the group used \gls{toggl} and had it integrated with \gls{jira}. This was a slight problem as it required the administrators of our \gls{jira} instance. We also required the administrators for \Gls{git}Hub integration, project setup and team management. It seemed that \gls{jira}, although useful, was not the correct tool for our use case as it made it harder for the team to be independent.

There was also a problem with the \Gls{git}Hub integration as it did not allow for syncing of the issue tracking. This was a problem because the team wanted to handle the project as an \gls{opensource} project and wanted the issues publicly available. An alternative would be to use \Gls{git}Hub with plug-ins for issue tracking and estimations.

Decisions made during development were recorded using \gls{confluence}. The export can be found in appendix \ref{appendix:decisionLog}.

\section{Quality metrics and measures}
\newglossaryentry{taintPropogation}{name={taint propagation}, description={Using data flow to determine what an attacker can control \cite{chess2007secure}}}
The quality metrics and measures were originally a very important aspect of the project, but were pushed back due to the complexity and how it required higher completeness of the \gls{antlr} parsing. The topic was researched and decided to focus on metrics mentioned in sub section \ref{subsection:SprintComplexParsingAndScopeVisual}.
For the \Gls{cyclomaticcomplexity}, counting the branching factor from conditionals and exit statements seemed like something that could be done and would give a significant benefit. This is due to how it gives good representation of complexity on functions, classes and the project. \href{https://github.com/SonarSource/sonarqube}{Sonarqube API} was considered as a way to add this into the project as mentioned in sub section \ref{subsection:SprintComplexParsingAndScopeVisual}, but workload was too great and this is therefore still in development.

\Gls{connacensemetrics}, dealing with implicit connections within algorithms and between variables, was considered the Holy Grail of helpful statistics, but the complexity and difficulty of calculating this meant that it was decided that the group would not try to add it. The group do not even know if it would be possible to calculate connascence, since it seems like something that would be more like a description in an oral conversation. For instance; the group could not find any reliable way to calculate the algorithmic connacence between a \gls{frontend} input validation system and \gls{backend} validation system. If the validation checks if a number is between 0 and 10, then the two validations can be written in different languages, have different names for the validated variable like "i" and "n". The group would have to know the \gls{url} the request is send to, where the back-end redirects to and how the back-end obtains the parameters. The group would have to know that it is a validation layer or that the validation is integrated into the behaviour. Despite this the group still found it useful for describing our own system and discussing potential refactoring. The team considered looking into \gls{taintPropogation} techniques to get a better understanding of this. The team was familiar with \Gls{taintPropogation} through the Software security course \cite{course:softSec} and knew it was outside the scope of this project.

\section{IaC}
\newacronym{cicd}{CI/CD}{Continuous Integration and Continuous Deployment \cite{mabl:cicd}}
\newglossaryentry{puppet}{name={Puppet}, description={A server configuration tool \cite{puppet}}}
\newglossaryentry{chef}{name={CHEF}, description={A configuration management tool \cite{chef}}}


As mentioned in section \ref{sec:methodology}, it was initially going to be a greater focus on \gls{devops}. Sadly, due to the amount of work required for each feature and continual underestimation of time required for stories and issues, the team could not finish the \gls{cicd} features. \gls{cicd} were concepts the group really wanted and looked forward to using, but the lacking knowledge of the specific tools needed for this meant that it had to be postponed to focus on development. This was a necessary decision, but still one that was hard to make and one the team did not want to make. The team intended to use \gls{openstack} with \gls{hot} in such a way that one could automatically tear down and rise virtual servers as was recommended \cite{morris2016infrastructure}. The team is using \gls{openstack} and \gls{hot}. The resulting topology of the architecture as shown in figure \ref{fig:serverNet} was somewhat unexpected and did not seem good due to the high amount of coupling, but still was fully functional and usable. 

\input{inc/figures/serverNetwork.tex}

The problem with using the stack for an automated process was that the team was unable to easily configure it together with \gls{jenkins} and \gls{docker}. The team knew that these services where often used together and with \gls{puppet} or \gls{chef} but were unable to find what tool was meant for what purpose. In the end the team used the \gls{hot} template tag to run a script for installing docker that would in turn setup \gls{jenkins} master and slave. Then \gls{jenkins} could be configured to handle pushes to \gls{git} repository and update the slaves. As the server configurations were incomplete, it was easier for each group member to host and test on their own local systems instead.

If the server configurations had been completed, the setup could have been used to ensure tests were run continuously and required for pull requests to be accepted. 

\section{Perspective on antlr}
\subsection{Overall reflection and experience}
Simple \Gls{antlr} workflow is explained in section \ref{sec:technicalBackEnd}. Each listener provides a context that represents a keyword within the file being parsed. The context is a sequential match of tokens. The number of tokens in a rule is the breadth of the context, whilst the depth of the context is the number of nested tokens before you reach a fragment that is a leaf node and a character sequence. The form of a token is an alternative rule that match the context.  

At the beginning of the project the group decided to parse some basic parts of the codebase to get familiar with \gls{antlr}, without thinking much about making it a proper system. This way the group was able to come up with a working prototype in the first week of development, but as the development progressed, the code would not scale due to a few tasks that required huge nested-if blocks to deal with the depth of the context. The problem with if statements is explained in sub section \ref{subsection:parsingJava}.

To retain the professionalism in the code, the group needed to find out a better way to use \gls{antlr} for code parsing. Through research and experiments the group came up with 3 solutions over time to structure the \gls{antlr} part of the codebase:
\begin{itemize}
    \item Stack
    \begin{itemize}
        \item In the \gls{antlr} repository it was recommended to use a stack like data structure to process the code \cite{github:stackAntlr}. Since \gls{antlr} parses the code sequentially, a stack could be used to store what is being parsed. This solved our problem with understanding the scope of a context by separating them. As \gls{antlr} runs on individual files, this approach was unable to identify scopes from different files. This was a problem with the C++ grammar as it sends include statements into a hidden channel. This meant that the group was unable to merge the different files into one project and connect structures and function calls declared in separate files.
    \end{itemize}

    \item Java Action
    \begin{itemize}
        \item Java Action was considered to help with contexts having large breadth and many forms where the context is known to have a wanted token. Actions would be for functionality to be added when entering or exiting a context. One could essentially say "When entering this scope; look for all instances of this token". It would return the wanted token no matter what sub-tree it was from, alleviating the need to check what form each nested context has. It would do this by setting an \gls{antlr} listener to call an Action that is overwritten when entering or exiting a scope.
    \end{itemize}

    \item \gls{antlr} visitors 
    \begin{itemize}
        \item The \gls{antlr} visitor were recommended to gain more control in traversing the parse-tree and are usually used if only sub-trees are required to be parsed. The project required many different components of the codebase to be parsed and listeners were therefor the best option for us, where it will go through every node in the parse tree. This way the listeners could sequentially parse every built-in keyword or user defined names from every line on the codebase.
    \end{itemize}
\end{itemize}

\subsection{Decreasing cyclomatic complexity in ANTLR}
\input{inc/figures/cppFunctionDefinationGraph.tex}
When parsing function definition, it became clear that the depth of the context caused significant complexity as on each level the form of the context had to be checked. This required significantly nested if structures.

The team solved this problem partially using a stack of models. A model represents a structure in the codebase; for instance functions, classes, namespaces, etc. The model is added to the stack when it's scope is entered and removed after it has completed. This way the team was able to use specific listeners provided by \gls{antlr} to parse the specific fields for the model. 

With the stack, more listeners could be implemented without worrying that a listener would be called at the wrong time and the context be added in the wrong scope. Instead of parsing function definition in depth, one could implement the relevant listeners within the context and they would automatically add their data to their parent model where the parent model was function definition.
In figure \ref{CPPFunctionDefinition}, implementing functiondefinition to enter a function model and implementing typespecifier to add its type to the parent would mean function model would have this without explicitly checking declspecifierseq and declspecifier. 

Implementation of listeners in the object-oriented design created a lot of dependency relationships, which increased \gls{connacensemetrics} of the codebase. As figure \ref{CPPFunctionDefinition} shows every node in depth is dependent on its parent node, in most cases some nodes share the same parent node. This made it very difficult to figure out what listener could be executed next. The stack was still considered a good alternative as it removed a lot of \gls{cyclomaticcomplexity} and meant that more of the parsing burden was dealt with, by \gls{antlr}. 

Another solution for this could be using \gls{antlr} visitors, where visitors gives much more control on which token to visit. This way the team could exclude irrelevant sub-trees and only visit the information about a specific model. Due to time limitation, visitors were not experimented with, but this could be useful for future refactorings.

\subsection{Use of regex}
\newglossaryentry{regularExpression}{name={regular expression}, description={A sequence of characters that define a search pattern \cite{wiki:regex}}}
\newacronym{regex}{regex}{\Gls{regularExpression}}

Handling function calls was difficult as in Java and C++, the call can be called on an object and that object can in turn be called on any other object recursively. The grammar did not easily allow for the separation of these objects as the languages are very versatile with what an object might be. To bypass the difficulty of using \gls{antlr} for this purpose, the group decided to use \gls{regex}. The \gls{regex} was used to split the call on "." and "->" that delimited the different objects while other string operations were used to remove the body of some brackets. \Gls{regex} could not remove the content of the brackets correctly as they could in turn contain brackets and \gls{regex} can not handle \gls{recursion}. The content of brackets were removed to make it easier to identify the associated function. This approach would not scale in the long run and is likely to create problems. The problems that can arise, come from how this approach prevents \gls{antlr} from taking care of the recursive structure and niche aspects of the languages that the team has not considered. Removing the body also prevents the system from knowing what overloaded function is being called. Using the \gls{antlr} built-in features would be preferable in the long term, but would require significantly more work. 

The \Gls{regex} approach could also be used to parse function pointer assignments as this was a problem due to use of symbol "*" as a pointer and as a multiplication sign. This made it impossible for the team to differentiate between pointer assignments and multiplication expressions. A long term solution to this problem could be updating the grammar file with a rule for pointer assignments, but this was not done since it would require in-depth understanding of the grammar files. Function pointers were dropped due to time limitations.

\section{Perspective on REST and WebSocket}
As mentioned in sub section \ref{subsection:Go}, the \gls{rest} requirement of the project had to be relaxed. The addition of \glspl{websocket} was a necessary decision to take and integrate into the system. \Gls{websocket} are built-in to \gls{js}, a easy-to-use library for Go made it quick to set up and worked well for the purpose.

\Gls{websocket} is a different protocol that uses \gls{http} requests to request a \Gls{websocket} connection and then switches over to \Gls{websocket} protocol if server allows. This allows messages to be sent back and forth over 3 different channels, "open", "close" and "message". The "open" channel is for when the \Glspl{websocket} opens, "close" is for receiving shutdown message for the connection and "message" is for any information that needs to be sent. 

One solution to improve the \gls{rest} portion of the \gls{api} is to offer \Glspl{websocket}, along with a \gls{rest} \gls{api}, which will give next to no information about the parsing progress, but returns the same result.

The use of \gls{websocket} in Go led to very huge functions and this resulted in decreased readability of the endpoints. Huge functions were the side effect of error handling of \gls{websocket}. To improve readability the team could break the the functionality of the endpoint into multiple small functions.

One mitigation for the massive amount of error handling would be to use a concept called middleware. This is when you have code that is executed before or after any given code snippet for a specific reason. For example one could wrap a log-in functionality in middleware that would write to a log-file. This could log the users attempt and subsequent success or failure of the log-in. As well as logging, middleware could be used for prepping or extracting input to sensitive functions ensuring no \gls{taintPropogation} and injections, as well as handling errors.

\section{Perspective on Graphics systems}
\subsection{Overall reflection and experience}
Selection of a graphics system was relatively easy. The whole group had experience with OpenGL from the Graphics programming course \cite{course:graphics} and \gls{webgl} is closely related. Everyone already knew the details on how to draw in 3D, but since it was a new language the group didn't have much experience in, the group setup a workshop to familiarize with \Gls{js} and \gls{webgl}. The group also found THREE.js which is a well-known and well-used graphics framework in \Gls{js}. THREE.js handles \gls{webgl} automatically, was easy to use, contains ready made shapes called geometries and have a lot of support functionality like vector classes with built-in math. The group therefore decided to use THREE.js.

The project required to show information and interactable \gls{ui}, for this reason the group researched different \gls{ui} Libraries/frameworks found PixiJS, TWO.js, dat.GUI and ImGUI-js.

\subsection{Choice of graphics framework}
Using THREE.js meant that the graphics handling was not a problem, but there were still a problem with using the parsed codebase to create the 3D representation. This was handled in \Gls{js}, although later it seemed like parts of it could be handled by the \gls{backend} and allow for an improved web \gls{api} and better caching opportunities. 

\subsection{Choice of UI framework}
The group thought about creating a custom \gls{ui} system using PixiJS or TWO.js instead of using other existing libraries. This seemed like a lot of management and adjustment work, considering it would only be used for the visualization. 

dat.GUI is a pure \Gls{js} and HTML \gls{ui} Library. Although it seems easy to use and could possibly work, it seemed too simple from available \href{https://workshop.chromeexperiments.com/examples/gui/#1--Basic-Usage}{examples}. This would require large amounts of configuration code and would stretch the library's capabilities, which wasn't really desired. Only scenarios where "dat.GUI" would fit well is for a settings or configurations drop-down menu.

ImGUI-js is a 3rd party library and must work in unison with ImGUI, but the difference from TWO.js and PixiJS is that ImGUI-js wrappes ImGUI, so in-effect they're one library and this made it very difficult to integrate, therefore lot of time went into making it work properly, but the group did not have to get two libraries to work at the same time. Another benefit of ImGUI is that the \gls{ui}-elements exists inside windows which are by default movable and scalable. This results into a very customizable \gls{ui} layout out of the box if this is wanted.

\subsection{Choice of Application Lifecycle Framework}
\newacronym{alf}{ALF}{Application Lifecycle Framework}

The group considered to use framework for \gls{frontend} development because the use of framework is often recommended, especially when using \Gls{js} due to the way it handles scopes. The supervisor recommended multiple frameworks such as POLYMER.js, VUE.js, AngularJS and REACT.js. The project is a web-based application which has only two HTML pages and most of the \gls{frontend} deals with graphics.

Considering this the team later realized that there was no need for it. Most \glspl{alf} are not designed for graphics and therefore did not fit well with this application. It could restrict and require time to learn without giving any significant benefits.

\subsection{Use of Force Directed Graph library}
There are a multitude of libraries out there that handles \gls{fdg} quite good like \href{https://www.npmjs.com/package/3d-force-graph}{3d-force-graph} and \href{http://getspringy.com/}{Springy.js}, but none of these libraries allowed for individual link strengths. They only have a single global setting for all attractive links, and the reason this was wanted was due to the different data structures being more or less attracted to other certain data structures. Therefore it was decided to not not integrate any \gls{fdg} library. 

\subsection{Graphics system refactoring}
The 3D representation was changed multiple times but not fully refactored so it still contains code that should have been deprecated, removed or changed to fit the rest of the system. Some of these changes were made late in the project when time-constraints were causing problems. The time-constrains meant that proper refactoring would not improve the velocity of the team and were therefore not prioritized.

\section{Perspective on FDG}
\subsection{Overall reflection and experience}

The codebase visualization had to be intuitive and able to handle both large and small datasets without any manual adjustment by the user or administrator. A \gls{fdg} seemed like a natural choice for automatically adjust the layout based on the scale and composition of the codebase. It also seemed natural to use the scoping in object oriented programming to hide or reveal details based on what the user was looking at.

As this was the first time the group had used \gls{fdg}, it was a surprise how flexible the literature was about the implementation specification. The literature helped establish a few terms:

\begin{itemize}
    \item Node - A representation of codebase structure with positional data and metadata requiered by \gls{fdg} to update position.
    \item Energy - relating to the total energy in the system and what the \gls{fdg}'s goal was to minimize.
    \item Attractiveness - metadata of node relating to how much energy is required to keep it apart from one other node.
    \item Repulsiveness - metadata of node relating to how much energy is required to keep it close to one other node.
    \item Links - metadata of node, a map of all repulsive and attractive forces to all other nodes.
    \item Gravity - A global force affecting all nodes, pulling them towards a defined center.
\end{itemize}

\subsection{Detailed experience on refactoring of FDG}
The \gls{fdg} is probably the part in the program that caused the most problem when trying to expand on it and refactor it.
It started off as a prototype with limitations:
\begin{itemize}
    \item Assumed no nested structures as output.
    \item Assumed a global center that all structures should gravitate towards to keep the graph centered.
    \item Assumed an initial repulsion from one structure to another, that's not related and of equal strength.
    \item Assumed the repulsive forces was negative by the algorithm.
    \item Assumed all repulsive forces were set as -1 by the parsing.
    \item Assumed a connection or attraction from structures based on nested structures in input.
\end{itemize}

The \gls{fdg} used Inverse-square-law to calculate force based on repulsiveness and Hooke's law to calculate a force from attractiveness. All the nodes were parsed from the initial nested codebase structure in pre-order and stored in a single array within the \gls{fdg} object.
The initial structure allowed for relatively simple parsing and well understood and fairly versatile structures. Initially it was desirable for nodes to contain minimal information and only the information required by \gls{fdg}. This would improve the object oriented model and minimize the amount of data being passed around. The desire to keep nodes minimal was lessened when the group looked into \gls{js} as a "Share by Calling" language. 

When it came to changing \gls{fdg} to allow for nested structures as output, it soon became clear that large portions would have to be changed. The wish was to have a system that would be streamlined and versatile. This meant to change array containing all the nodes into a generic tree that more closely represented both the input and output data. It was required to change the single array approach as allowing for nested output, which meant the \gls{fdg} would have to run on each nested part separately. Changing the array proved complicated, as links were added while parsing the input and used the index in the array to identify the other node, with a tree, the index would have to be calculated and kept constant. It was thought that keeping the index constant could only be done by parsing the input level-order or pre-order, and this would not allow for nested output as the size of each node was dependent on its children. Therefore the parsing was done post-order and contained a local index that could be used to calculate the global index in the tree when connected to the root. This result is described in sub section \ref{subsubsec:recuriviryOfNode}. This assumption turned out to be wrong, and giving the nodes a global index when building the tree would have been beneficial.

The local to global calculations caused a lot of confusion and significantly increased complexity. The new system had the same cyclomatic complexity as before, keeping conditionals essentially the same, but increased the use of complicated \gls{recursion} and had a high connacance with the removed array through the global index.

Although there were made mistakes during the refactoring and it increased the complexity, it was still beneficial. Benefits of using the tree structure were evident, as the group could run \gls{fdg} on each node separately from the world origin and input this to the \gls{scenegraph} to handle offsets and potential navigation. From parsing in the \gls{backend} to the visualization in \gls{frontend}, the core data is always handled as a tree structure. This means there is a high level of consistency throughout the system but also an implicit connacance that means if the structure is changed in one part of the system then all other parts will also have to be changed.

\section{"sloccount" vs "Kloc" vs "wc -l"}
One of the simplest complexity metrics to add was \gls{loc} and was added as a first, simple metric. Sloccount, \gls{kloc} and wc are tools that can get the \gls{loc}. wc was chosen. 
wc is a linux command line tool to read words, newlines and bytes and prints out the result to standard output channel. wc is language independent meaning it can take any language file as an argument. It can also read only the specified lines within the files by giving the "-l" flag when executing. This worked well for our purpose and therefore used to fetch the implementation from source code files. The other alternatives were considered because wc would not be able to distinguish between comments, code or empty lines, but the other alternatives had the same limitations or would not support enough languages for our purpose. 

\section{Platform for the product}
The reason of choosing web as the platform for the the system was for it to be cross-platform and so that any user could easily access the system from anywhere. Also the team did not have much experience in the web-technology therefore it was a good opportunity to implement the system as web-service.
\gls{js} is the technology the team didn't not have much experience with and become a problem to scale as responsibilities of \gls{frontend} increased over time.

\section{Post-phoning/removing userstory allowing user to submit lexer files}
There was an idea early on that the user would be able to input a lexer file for any language and be able to use it to visualize projects written with-in that language. It became clear very quickly the more the team worked with \gls{antlr} that this feature was not going to be integrated. The listener files are generated based on the grammar file and are language specific, which means one would have to implement each listener independently for each language for the system to be able to parse that language. So to integrate the submitted lexer files, the application have to either inject the new code in during runtime which is far beyond the project scope. It was decided to drop this functionality due to the shear size of this feature, which on its own can be counted as a bachelor if not even more.